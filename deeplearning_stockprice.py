# -*- coding: utf-8 -*-
"""deeplearning_stockprice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yLJAancZ8XGKFCa-xXTqlaQz3zUWxrAp

**Asa Wilks, 8/2/2023**

**Deep Learning Stock Price Predictor: FY23 Getting Smart(er) About AI Capstone**

This project ingests current asset price data from the Yahoo Finance API and predicts stock closing price based on past price movement. The deep learning model is a recurrent nueral network with long short-term memory (LSTM) layers. If you are using Goolge Colab don't forget to turn on the free GPU in the notebook, it will run much much faster.
"""

##
# imports
##

import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
from sklearn.preprocessing import MinMaxScaler

import math
import numpy as np
import pandas as pd
import yfinance as yf

import matplotlib.pyplot as plt
plt.style.use('seaborn-v0_8-pastel')

"""I use the Vanguard Total Stock Market Index Fund ETF, but any ticker symbol can be subbed in for 'VTI' in the code below in order to train a model for a different asset. The API loads daily data for the window specified. Currently this tool only uses past fluctuation in close price to predict future close, but a next step would be to add other stock data such as volume and price-earnings ratio, as well as macroeconomic variables like CPI and unemployment."""

##
# Read the data for Vanguard Total Stock Market Index Fund ETF
# Keep just the close prices for now
##

symbol = yf.Ticker("VTI").history(period="5y")

# save the price values to a numpy array
close_symbol = symbol.filter(['Close'])
close_symbol_arr = close_symbol.values

symbol

"""Definitions of the length of lookback (in days) and the proportion of the data to be used as training (vs test/validation)."""

# define crossvalidation split and number of days for training and
lookback = 60
train_size = int(np.round(len(close_symbol_arr) * .8))

"""The full data is limited to the training set and reshaped as needed for the neural network.  This means each close price has all days in the lookback period as predictors, which creates bigger data but (according to Tensorflow) is faster than trying to loop back across the days in each step."""

##
# training set -  80% of the data
##

# normalize the data on scale of 0 to 1
sc = MinMaxScaler(feature_range=(0, 1))
close_symbol_arr_scaled = sc.fit_transform(close_symbol_arr)
train_scaled = close_symbol_arr_scaled[0:train_size, :]

# create data over the lookback window
x_train_scaled = [] # empty list
y_train_scaled = [] # empty list

for i in range(lookback, len(train_scaled)):
    x_train_scaled.append(train_scaled[i-lookback:i, 0])
    y_train_scaled.append(train_scaled[i, 0])

# reshape
x_train_scaled = np.array(x_train_scaled)
y_train_scaled = np.array(y_train_scaled)
x_train_scaled = np.reshape(x_train_scaled, (x_train_scaled.shape[0], x_train_scaled.shape[1], 1))

"""Validation and test data are pooled together here because time is limited. If iterative experimentation with network architecture or hyperparameter tuning was on the agenda, validation and test sets should be separate (maybe a 80/10/10 split) with the validation versions specified in model.fit()."""

##
# validation + test set - 20% of the data
##

test_scaled = close_symbol_arr_scaled[train_size - lookback:, :]

x_test_scaled = []
y_test = close_symbol_arr[train_size:, :]
y_test_scaled = close_symbol_arr_scaled[train_size:, :]

for i in range(lookback, len(test_scaled)):
    x_test_scaled.append(test_scaled[i-lookback:i, 0])

# reshape the data
x_test_scaled = np.array(x_test_scaled)
x_test_scaled = np.reshape(x_test_scaled,
                           (x_test_scaled.shape[0],
                            x_test_scaled.shape[1], 1))

"""The network has two LSTM layers with 96 and 48 nodes, respectively, with a dense layer before the output layer.  Dropout seemed better at preventing overfitting than using kernel/recurrent regularizers.  The final model has 66,673 parameters.  Very little time was spent exploring archetecture/hyperparameter choices, there is likely plenty of room for improvement.  Don't forget to separate the test/validation sets before doing that."""

##
# build the lstm network
##

model = Sequential()
model.add(LSTM(units=96,
               #kernel_regularizer = tf.keras.regularizers.L1(0.0005),
               return_sequences=True,
               input_shape=(x_train_scaled.shape[1],1)))
model.add(Dropout(.5))
model.add(LSTM(units=48,
               return_sequences=False))
model.add(Dropout(.5))
model.add(Dense(24))
model.add(Dense(1))

model.summary()

"""The loss function chosen was MSE and the optimizer was Adam. The learning rate was 0.0005, half the default, to train more slowly."""

##
# compile and fit the lstm
##

optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)
model.compile(loss='mean_squared_error', optimizer=optimizer)

model.fit(x_train_scaled, y_train_scaled,
          epochs=50,
          batch_size=16,
          #validation_data = (x_val_scaled, y_val_scaled),
          verbose=True)

##
# make predictions based on the test set
##

preds = model.predict(x_test_scaled)
preds = sc.inverse_transform(preds)

# rmse
rmse = np.sqrt(np.mean(((preds - y_test)**2)))
rmse

"""The plot below shows the predictions for the test set overlaid on the actual values. The results look promising, but we haven't tested this against any kind of benchmark. For exanple how does this model do compared to simply predicting no change in previous day close price? It would be interestig to compare with the standard time series models as well (e.g. ARIMA, etc)."""

##
# plot the actual values vs predictions
##

train = close_symbol[:train_size]
valid = close_symbol[train_size:]
valid['Predictions'] = preds

plt.figure(figsize=(16,8))
plt.title('Model')
plt.xlabel('Date', fontsize=18, fontweight='bold')
plt.ylabel('Close Price ($)', fontsize=18, fontweight='bold')
plt.plot(train['Close'], color='green')
plt.plot(valid[['Close', 'Predictions']])
plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')
plt.show()

"""Finally, if the model was going to be used for trading decisions (not recommended!) it needs to predict tomorow's (currently unobserverd) closing price. It predicts the closing price for 8/22/23 to be 225.74. In reality, makets closed down sharply overall with VTI falling to 224.50."""

##
# predict for next day (currently unobserved)
#

tmrw_x_scaled = []
tmrw_x_scaled.append(close_symbol_arr_scaled[len(close_symbol_arr_scaled)-lookback:len(close_symbol_arr_scaled), 0])
tmrw_x_scaled = np.array(tmrw_x_scaled)

tmrw_x_scaled = np.reshape(tmrw_x_scaled,
                             (tmrw_x_scaled.shape[0],
                             tmrw_x_scaled.shape[1], 1))

pred_tmrw = model.predict(tmrw_x_scaled)
pred_tmrw = sc.inverse_transform(pred_tmrw)

print(pred_tmrw)

!pip install nbconvert